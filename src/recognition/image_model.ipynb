{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Set the directories for training and validation data\n",
    "# train_dir = 'data/train'\n",
    "train_dir = 'E:/Projects/Sign Language Project/SignSpeak/data/raw'\n",
    "# val_dir = 'data/val'\n",
    "# val_dir = 'E:/Projects/Sign Language Project/SignSpeak/data/raw/test'\n",
    "val_dir = 'E:/Projects/Sign Language Project/SignSpeak/data/ASL/train_reduced10'\n",
    "\n",
    "# Image size\n",
    "# IMAGE_SIZE = (64, 64)  # Adjust based on the input requirements of your model\n",
    "IMAGE_SIZE = (200, 200)  # Adjust based on the input requirements of your model\n",
    "BATCH_SIZE = 32\n",
    "NUM_FRAMES = 30\n",
    "\n",
    "# Create an ImageDataGenerator for preprocessing images\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255,  # Normalize the pixel values to [0, 1]\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "# Load the images in batches directly from the directory\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Get the number of classes\n",
    "num_classes = len(train_generator.class_indices)\n",
    "\n",
    "print(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model design and image sequence generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, LSTM, Dense, TimeDistributed, Dropout\n",
    "\n",
    "# Define CNN-LSTM model\n",
    "def build_cnn_lstm_model(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "\n",
    "    # TimeDistributed CNN to process image sequences\n",
    "    model.add(TimeDistributed(Conv2D(32, (3, 3), activation='relu'), input_shape=input_shape))\n",
    "    model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "    model.add(TimeDistributed(Conv2D(64, (3, 3), activation='relu')))\n",
    "    model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "    model.add(TimeDistributed(Conv2D(128, (3, 3), activation='relu')))\n",
    "    model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "    # LSTM layer to capture temporal dependencies\n",
    "    model.add(LSTM(64, return_sequences=False))\n",
    "\n",
    "    # Dense layers\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# # Define the necessary variables\n",
    "# NUM_FRAMES = 30  # Example value, set according to your dataset\n",
    "# IMAGE_SIZE = (64, 64)  # Example value, set according to your dataset\n",
    "# num_classes = 10  # Example value, set according to your dataset\n",
    "\n",
    "\n",
    "def image_sequence_generator(directory, batch_size, target_size, num_frames):\n",
    "    \"\"\"\n",
    "    Custom data generator to yield batches of image sequences.\n",
    "    \"\"\"\n",
    "    class_folders = os.listdir(directory)\n",
    "    num_classes = len(class_folders)\n",
    "\n",
    "    while True:\n",
    "        X_batch = []\n",
    "        y_batch = []\n",
    "\n",
    "        for _ in range(batch_size):\n",
    "            # Randomly select a class folder\n",
    "            class_folder = np.random.choice(class_folders)\n",
    "            class_index = class_folders.index(class_folder)\n",
    "            class_path = os.path.join(directory, class_folder)\n",
    "\n",
    "            # Randomly select sequence of images\n",
    "            image_files = os.listdir(class_path)\n",
    "            selected_images = np.random.choice(\n",
    "                image_files, num_frames, replace=False)\n",
    "\n",
    "            # Load and preprocess each image in the sequence\n",
    "            image_sequence = []\n",
    "            for image_file in selected_images:\n",
    "                image_path = os.path.join(class_path, image_file)\n",
    "                image = load_img(image_path, target_size=target_size)\n",
    "                image = img_to_array(image) / 255.0  # Normalize pixel values\n",
    "                image_sequence.append(image)\n",
    "\n",
    "            # Stack sequence and add to batch\n",
    "            X_batch.append(np.stack(image_sequence))\n",
    "            y_batch.append(class_index)\n",
    "\n",
    "        # Convert batches to numpy arrays\n",
    "        X_batch = np.array(X_batch)\n",
    "        y_batch = np.array(y_batch)\n",
    "\n",
    "        # Convert labels to categorical\n",
    "        y_batch = tf.keras.utils.to_categorical(\n",
    "            y_batch, num_classes=num_classes)\n",
    "\n",
    "        yield X_batch, y_batch\n",
    "\n",
    "\n",
    "# Parameters\n",
    "num_frames = 10  # Number of frames in each sequence\n",
    "batch_size = 32\n",
    "target_size = IMAGE_SIZE  # Same as used earlier (e.g., (64, 64))\n",
    "\n",
    "# Create generators for training and validation\n",
    "train_seq_generator = image_sequence_generator(\n",
    "    train_dir, batch_size, target_size, num_frames)\n",
    "val_seq_generator = image_sequence_generator(\n",
    "    val_dir, batch_size, target_size, num_frames)\n",
    "\n",
    "\n",
    "# Define input shape: (num_frames, image_height, image_width, num_channels)\n",
    "input_shape = (NUM_FRAMES, IMAGE_SIZE[0], IMAGE_SIZE[1], 3)  # 3 channels (RGB)\n",
    "\n",
    "# Build the model\n",
    "model = build_cnn_lstm_model(input_shape, num_classes)\n",
    "\n",
    "# Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the CNN-LSTM model\n",
    "history = model.fit(\n",
    "    train_seq_generator,\n",
    "    validation_data=val_seq_generator,\n",
    "    epochs=30,\n",
    "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
    "    validation_steps=val_generator.samples // BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Save the model after training\n",
    "model.save('models/sign_language_cnn_lstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the validation set\n",
    "val_loss, val_accuracy = model.evaluate(\n",
    "    val_seq_generator, steps=val_generator.samples // BATCH_SIZE)\n",
    "\n",
    "print(f\"Validation Loss: {val_loss}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
